
# Executor: switch from slurm plugin to cluster-generic
executor: cluster-generic

# Concurrency (same as before)
jobs: 10

# Software environments (Snakemake â‰¥ 8 prefers software-deployment-method)
# Equivalent to old "use-conda: True"
software-deployment-method: conda

# Default resources (carried over from your SLURM profile)
default-resources:
  mem_mb: 20000                # memory for the whole job
  runtime: 120                 # minutes or HH:MM:SS depending on your cluster
  slurm_partition: PARITION     # keep the same resource key name
  slurm_account: ACCOUNT
  ntasks: 1

# Submit/status/cancel commands required by cluster-generic
# We pass your resources to sbatch and make output/err files similar to your log layout.
cluster-generic-submit-cmd: >-
  sbatch
  --account={resources.slurm_account}
  --partition={resources.slurm_partition}
  --time={resources.runtime}
  --cpus-per-task={threads}
  --mem={resources.mem_mb}
  --ntasks={resources.ntasks}
  --parsable
  --output=generic/logs/%j.out
  --error=generic/logs/%j.err

# Provide a status command (script shown below). If you prefer, you can
# point this to any script/binary that accepts a single jobid argument.
cluster-generic-status-cmd: "~/bin/status.py"

# Cancel is simply scancel in SLURM
cluster-generic-cancel-cmd: "scancel"

# Retries / reruns (unchanged)
restart-times: 0

# If you want to keep the same log directory as the SLURM plugin used,
# ensure the folder exists before submission (sbatch won't create it).
# mkdir -p slurm/logs
